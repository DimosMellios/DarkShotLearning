{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import glob\n",
    "from tqdm.auto import tqdm\n",
    "import cv2\n",
    "import re\n",
    "import random\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from PrepSteps.ReSize import FindSize ,SizeScaler\n",
    "from PrepSteps.OneShotAugment import FindMax , AugmentData\n",
    "from PrepSteps.PairGen import make_pairs\n",
    "from PrepSteps.PrepareData import PrepareData\n",
    "from LoadModels.SiameseNet import SiameseNet\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, auc, classification_report\n",
    " \n",
    "\n",
    "from tensorflow.keras.layers import Activation,Input,Lambda, Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DIRECTORIES\n",
    "\n",
    "data_path = r'DIRECTORY OF THE DATASET'\n",
    "\n",
    "# NEW DIRECTORY\n",
    "rem = len(data_path) - len(data_path.split('\\\\')[-1]) - 1\n",
    "new_path = data_path[:rem]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_____________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Preparation of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The least acceptable images in each category\n",
    "thrush = 42\n",
    "## The final size of the images\n",
    "rescaler = (100,100)\n",
    "\n",
    "\n",
    "#### All the preparation steps in a sequence\n",
    "PrepareData(data_path,image_size = rescaler, thrush,left_overs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### If you do not need all the preparation steps uncomment bellow"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Provides information of the image dimensions\n",
    "FindSize(data_path)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Reshapes the images in the folders\n",
    "SizeScaler(data_path,thresh_size = None,final_size = (100,100))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Relocates the folders and keeps the folders with at least * images\n",
    "FindMax(data_path,10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Balanced augmentation steps (6 augmentations)\n",
    "AugmentData(new_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Siamese Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(data, location):\n",
    "    \n",
    "    '''Creates a dataframe with the location and\n",
    "    the category + sub-category information'''\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    df.columns = ['Path']\n",
    "    df['Category'] = df['Sub-Category'] = ''\n",
    "\n",
    "    ## Split in categories and sub-categories\n",
    "    for i in range(len(df)):\n",
    "\n",
    "        tmp_categories = df['Path'][i].split(location)[1].split(\"\\\\\")[1:3]\n",
    "        df['Category'].iat[i] = tmp_categories[0]\n",
    "        df['Sub-Category'].iat[i] = tmp_categories[1]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "def DataLoad(location): #, size\n",
    "    \n",
    "    # find the size of our dataset\n",
    "    data = glob.glob(location+'\\\\***\\\\**\\\\*.png')\n",
    "    \n",
    "    # create the dataframe with the categories and shuffle the data\n",
    "    info = counter(data,location)\n",
    "#     info = shuffle(info)\n",
    "    \n",
    "    # define the size of train and test \n",
    "#     x_size = int(len(info) * (1 - split_size))\n",
    "#     y_size = 1 - x_size\n",
    "    \n",
    "    # Encode the Labels of the dataset\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    \n",
    "    info['Label'] = le.fit_transform(list(info['Sub-Category'])) \n",
    "#     print(info['Label'],info['Sub-Category'])\n",
    "    ## Labels to array\n",
    "    labels = np.array(info['Label'])\n",
    "    \n",
    "    images = []\n",
    "    # Append the images to X\n",
    "    for i in info['Path']:\n",
    "        image = cv2.imread(i)\n",
    "        images.append(image)\n",
    "    # Change the list to numpy array\n",
    "    images = np.asarray(images) \n",
    "    \n",
    "    return images, labels, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y ,dataset= DataLoad(PATH)\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loads the Siamese Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "siamese_net = SiameseNet((100,100,3))\n",
    "\n",
    "# create the pairs: split_size, num_pairs\n",
    "pairs_train, labels_train , pairs_test, labels_test= make_pairs(x,y,.3, 20)\n",
    "\n",
    "siamese_net.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create folder to save weights for later use\n",
    "if not os.path.exists('weights'):\n",
    "    os.makedirs('weights'\n",
    "\n",
    "                \n",
    "# monitoring = [tf.keras.callbacks.EarlyStopping(monitor = 'val_accuracy', patience=20)]\n",
    "\n",
    "siamese = siamese_net.fit([pairs_train[:,0].reshape(-1,100,100,3),pairs_train[:,1].reshape(-1,100,100,3)], labels_train[:],\n",
    "        batch_size=100,\n",
    "        epochs=100,\n",
    "        verbose=1,\n",
    "        validation_data=([pairs_test[:,0].reshape(-1,100,100,3),pairs_test[:,1].reshape(-1,100,100,3)], labels_test[:]))\n",
    "        #callbacks= monitoring)\n",
    "                \n",
    "                \n",
    "## Save the weights in the folder\n",
    "siamese_net.save_weights('weights/model_20Shot.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the accuracy and loss of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLot of loss and accuracy\n",
    "acc = siamese.history['acc']\n",
    "val_acc = siamese.history['val_acc']\n",
    "loss = siamese.history['loss']\n",
    "val_loss = siamese.history['val_loss']\n",
    "\n",
    "epochs = range(len(acc))\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training accuracy')\n",
    "plt.plot(epochs, val_acc, 'r', label='Validation accuracy')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the pairs for the test, similar to the training data\n",
    "pairs_test, labels_test , your, no= make_pairs(x_test,y_test,0.5,20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### If your GPU is exchausted uncomment bellow to keep a small sample"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pairs_test= pairs_test[:934]\n",
    "labels_test = labels_test[:934]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Cast the input data for the testing\n",
    "left = K.cast(pairs_test[:,0],dtype=float)\n",
    "right = K.cast(pairs_test[:,1],dtype=float)\n",
    "\n",
    "### predict on the model\n",
    "pred = siamese_net.predict([left, right],steps=1)\n",
    "\n",
    "\n",
    "def compute_accuracy(y_true, y_pred):\n",
    "    '''Compute classification accuracy with a fixed threshold on distances.\n",
    "    '''\n",
    "    pred = y_pred.ravel()>.5\n",
    "    return np.mean(pred == y_true)\n",
    "\n",
    "print(\"The accuarcy of the model is\",round(compute_accuracy(targets_test,pred)*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ROC Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fpr, tpr, thresholds = roc_curve(targets_test, pred)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=1, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the confusion matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_fixed = pred.ravel()>.5\n",
    "cmf= (confusion_matrix(targets_test,pred_fixed))\n",
    "\n",
    "# df_cm = pd.DataFrame(array, range(6), range(6))\n",
    "plt.figure(figsize=(10,7))\n",
    "sn.set(font_scale=1.4) # for label size\n",
    "sn.heatmap(cmf, annot=True,fmt='d', cmap=\"YlGnBu\", annot_kws={\"size\": 16}) # font size\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Precision and Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(targets_test, pred_fixed))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
